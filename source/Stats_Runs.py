import os
os.environ["OMP_NUM_THREADS"] = "4"
import numpy as np
import matplotlib.pyplot as plt
# plt.style.use('dark_background')
import h5py
import skopt
import scipy as sc
from skopt.space import Real
from skopt.learning import GaussianProcessRegressor as GPR
from skopt.learning.gaussian_process.kernels import Matern, WhiteKernel, Product, ConstantKernel
exec(open("Val_Functions.py").read())
exec(open("Functions_Test.py").read())
import matplotlib as mpl
import math
mpl.rc('text', usetex = True)
mpl.rc('font', family = 'serif')


#### Dataset loading

Ndim      = 9
idx       = range(Ndim)

Re        = 400
t_lyap    = 0.0163**(-1)    # Lyapunov time

downsample  = 4

hf       = h5py.File('./data/MFE_Sri_RK4_dt=0.25_'+str(Re)+'kt=048.h5','r')
UU       = np.array(hf.get('q'))[:,::downsample]
hf.close()


N1_val = 10 #number of time series used during training

N0  = UU.shape[0]
N1  = UU.shape[1]
U   = UU.reshape(N0*N1, Ndim)
print(UU.shape, U.shape)

#### Adding noise

# Set a target SNR in decibel
target_snr_db = 40
sig_avg_watts = np.var(U,axis=0) #signal power
sig_avg_db = 10 * np.log10(sig_avg_watts) #convert in decibel
# Calculate noise, then convert to watts
noise_avg_db = sig_avg_db - target_snr_db
noise_avg_watts = 10 ** (noise_avg_db / 10)
# Generate an sample of white noise
mean_noise = 0
noise_volts = np.zeros(U.shape)
seed = 0                        #to be able to recreate the data
rnd  = np.random.RandomState(seed)
for i in range(Ndim):
    noise_volts[:,i] = rnd.normal(mean_noise, np.sqrt(noise_avg_watts[i]),
                                       U.shape[0])
UU  = U + noise_volts

UU  = UU.reshape(N0,N1,Ndim)
kinetic = 0.5*np.linalg.norm(U,axis=1)**2
print(np.mean(kinetic), np.std(kinetic))

#### data management

dt        = .25*downsample  # timestep 
N_lyap    = int(t_lyap/dt)  # number of time steps in one Lyapunov time
print(N_lyap, dt)

# number of time steps for washout, train, validation, test
N_washout = N_lyap
N_val     = 2*N_lyap
N_train   = N1 - N_val - N_washout # 196*N_lyap
print(N_train/N_lyap)

#compute norm
U_data = U[:N_washout+N_train+N_val]
m = U_data.min(axis=0)
M = U_data.max(axis=0)
norm = M-m

# washout
U_washout = UU[:N1_val,:N_washout]
# training
U_t   = UU[:N1_val,N_washout:N_washout+N_train-1]
Y_t   = UU[:N1_val,N_washout+1:N_washout+N_train]
# training + validation
U_tv  = UU[:N1_val,N_washout:N_washout+N_train+N_val-1]
Y_tv  = UU[:N1_val,N_washout+1:N_washout+N_train+N_val]
NN    = N1_val
k_tv  = 0.5*np.linalg.norm(Y_tv[:NN].reshape(NN*(N_train+N_val-1), Ndim), axis=1)**2
# validation
Y_v  = UU[:N1_val,N_washout+N_train:N_washout+N_train+N_val]

#### ESN Initiliazation Parameters
bias_in = .1 #input bias
bias_out = 1.0 #output bias 
dim = Ndim # dimension of inputs (and outputs) 
connectivity   = 20 


#### Compute Statistics
def Stats(x, Wout, N_length, N_val, N_test):
    """Compute N_test cloop predictions that last N_length to compute the statistics generated by the ESN"""
    
    #hyperparameters
    global rho, sigma_in
    rho      = x[0]
    sigma_in = x[1] 
                
    # first instant to start the prediction from    
    N_tstart = N_washout + 1
    
    # initialize    
    Yh_t     = np.zeros((N_test,N_length,Ndim))
    k       = 0
    N_step  = int(N_lyap) #time difference between start of different predictions
    
    #Different Folds in the cross-validation
    for i in range(N_test):
        
        # data for washout and target in each interval
        U_wash    = U[N_tstart - N_washout +i*N_step : N_tstart+i*N_step]
        
        #washout for each interval
        xa1       = open_loop(U_wash, np.zeros(N_units), sigma_in, rho)[-1]
        
        # Prediction (this doesn't count the fact that we flattened the different time series into one, but if the washout is not done properly
        # and the network diverges, it is counted as a laminarized time series and discarded)
        Yh_t[i]   = closed_loop_stats(N_length-1, xa1, Wout, sigma_in, rho)[0]
        kin       = 0.5*np.linalg.norm(Yh_t[i],axis=1)**2
        
        # keeps track on how mani have laminarized
        if kin.max()>0.48:
            k +=1
   
        if (i%10) == 0 and i>0:
            print('Laminarized', k/i, i)

        
    return Yh_t



N_test    = 500 #number of time series for the ESN
N_length1 = N1  #length of each ESN time series

# to run multiple validation strategies and sizes of the reservoir in one go
NN_units = [500] #units in the reservoir
val      = [RVC]

restart  = [False for i in range(20)]
restart[0] = False #restart flag to start the run again from when it was interrupted

k        = 0

for N_units in NN_units:
    
    sparseness =  1 - connectivity/(N_units-1) 

    for jj in val:

        #Validation hyperparameters
        hf       = h5py.File('./data/Lor_short_'+ str(Re) + '_' + str(target_snr_db) + '_'  + jj.__name__ + '_' + str(idx) + '_4_' + str(N_units) +
                             '3LT_200LT_Multi.h5','r')
        Min      = np.array(hf.get('minimum'))
        Min[:,:2] = 10**Min[:,:2] #were computed in log scale
        hf.close()
        print(Min)

        # to save and then restart if time is up
        if restart[k]:
            fln       = './data/'+ str(Re) + '_' + jj.__name__ + '_Statss_' + str(N1_val) + '_' + str(N_units) + '.h5'
            hf        = h5py.File(fln,'r')
            Y_stats   = np.array(hf.get('Y_stats'))
            I         = np.array(hf.get('I'))
            hf.close()
        else:
            Y_stats  = np.zeros((Min.shape[0], N_test, N_length1, Ndim))
            I         = 0

        k += 1

        for i in range(I,Min.shape[0]):
             
            print('Ensemble    :',i+1)
                
            # Win and W generation
            seed= i+1
            rnd = np.random.RandomState(seed)

            Win = np.zeros((dim+1, N_units))
            for j in range(N_units):
                Win[rnd.randint(0, dim+1),j] = rnd.uniform(-1, 1) #only one element different from zero per row
            
            # practical way to set the sparseness
            W = rnd.uniform(-1, 1, (N_units, N_units)) * (rnd.rand(N_units, N_units) < (1-sparseness))
            spectral_radius = np.max(np.abs(np.linalg.eigvals(W)))
            W /= spectral_radius #scaled to have unitary spec radius

            # train the network
            rho      = Min[i,0]
            sigma_in = Min[i,1]
            tikh     = Min[i,2]
            Wout1     = train(U_washout, U_tv, Y_tv, tikh, sigma_in, rho)[1]
            
            #generate long time series
            print(N_length1, N1, N_test)
            Y_stats[i] = Stats(Min[i,:2], Wout1, N_length1, N1, N_test)

            fln = './data/'+ str(Re) + '_' + jj.__name__ + '_Statss_' + str(N1_val) + '_' + str(N_units) + '.h5'
            hf = h5py.File(fln,'w')
            hf.create_dataset('Y_stats'   ,data=Y_stats)
            hf.create_dataset('I'      ,   data=i+1)
            hf.close()